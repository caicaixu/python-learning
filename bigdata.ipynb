{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 创建一个示例DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 注册DataFrame为临时视图\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# 使用SQL查询\n",
    "result = spark.sql(\"SELECT Name, Age FROM people WHERE Age > 30\")\n",
    "\n",
    "# 显示查询结果\n",
    "result.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/arwen/anaconda3/lib/python3.11/site-packages/dtkApi-1.0.10-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: kafka-python in /Users/arwen/anaconda3/lib/python3.11/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "<!-- Note: you may need to restart the kernel to use updated packages. -->\n",
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "### 如何本地安装Kafka环境\n",
    "\n",
    "1. **下载Kafka**:\n",
    "    - 访问 [Kafka官网](https://kafka.apache.org/downloads) 下载最新版本的Kafka。\n",
    "    - 选择一个镜像站点并下载Kafka的二进制文件（例如：`kafka_2.13-2.8.0.tgz`）。\n",
    "\n",
    "2. **解压文件**:\n",
    "    ```bash\n",
    "    tar -xzf kafka_2.13-2.8.0.tgz\n",
    "    cd kafka_2.13-2.8.0\n",
    "    ```\n",
    "\n",
    "3. **启动ZooKeeper**:\n",
    "    Kafka依赖于ZooKeeper来管理集群。首先需要启动ZooKeeper。\n",
    "    ```bash\n",
    "    bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "    ```\n",
    "\n",
    "4. **启动Kafka服务器**:\n",
    "    在另一个终端窗口中启动Kafka服务器。\n",
    "    ```bash\n",
    "    bin/kafka-server-start.sh config/server.properties\n",
    "    ```\n",
    "\n",
    "5. **创建主题**:\n",
    "    创建一个名为`my_topic`的主题。\n",
    "    ```bash\n",
    "    bin/kafka-topics.sh --create --topic my_topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
    "    ```\n",
    "\n",
    "6. **生产消息**:\n",
    "    启动一个Kafka生产者并发送消息。\n",
    "    ```bash\n",
    "    bin/kafka-console-producer.sh --topic my_topic --bootstrap-server localhost:9092\n",
    "    > Hello, Kafka!\n",
    "    ```\n",
    "\n",
    "7. **消费消息**:\n",
    "    启动一个Kafka消费者来消费消息。\n",
    "    ```bash\n",
    "    bin/kafka-console-consumer.sh --topic my_topic --from-beginning --bootstrap-server localhost:9092\n",
    "    ```\n",
    "\n",
    "通过以上步骤，你已经在本地成功安装并运行了Kafka环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# 生产消息\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# 发送消息\n",
    "future = producer.send('my_topic', b'Hello, Kafka!')\n",
    "\n",
    "# 检查发送结果\n",
    "try:\n",
    "    record_metadata = future.get(timeout=10)\n",
    "except KafkaError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    pass\n",
    "\n",
    "# 消费消息\n",
    "consumer = KafkaConsumer('my_topic', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest')\n",
    "\n",
    "for message in consumer:\n",
    "    print(f\"Received message: {message.value.decode('utf-8')}\")\n",
    "    break  # 只消费一条消息后退出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "\n",
    "# 创建TableEnvironment\n",
    "env_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# 创建Kafka连接表\n",
    "kafka_source_ddl = \"\"\"\n",
    "CREATE TABLE kafka_source (\n",
    "    `user_id` STRING,\n",
    "    `item_id` STRING,\n",
    "    `behavior` STRING,\n",
    "    `ts` TIMESTAMP(3)\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'my_topic',\n",
    "    'properties.bootstrap.servers' = 'localhost:9092',\n",
    "    'format' = 'json',\n",
    "    'scan.startup.mode' = 'earliest-offset'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 注册Kafka连接表\n",
    "table_env.execute_sql(kafka_source_ddl)\n",
    "\n",
    "# 查询Kafka连接表\n",
    "result_table = table_env.sql_query(\"SELECT user_id, item_id, behavior, ts FROM kafka_source WHERE behavior = 'buy'\")\n",
    "\n",
    "# 将结果打印到控制台\n",
    "table_env.to_append_stream(result_table).print()\n",
    "\n",
    "# 执行Flink作业\n",
    "table_env.execute(\"Flink SQL Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamparse import Topology, Spout, Bolt\n",
    "from streamparse.storm import Tuple\n",
    "import random\n",
    "from streamparse import run\n",
    "\n",
    "class RandomSentenceSpout(Spout):\n",
    "    outputs = ['sentence']\n",
    "\n",
    "    def initialize(self, stormconf, context):\n",
    "        self.sentences = [\n",
    "            \"the cow jumped over the moon\",\n",
    "            \"an apple a day keeps the doctor away\",\n",
    "            \"four score and seven years ago\",\n",
    "            \"snow white and the seven dwarfs\",\n",
    "            \"i am at two with nature\"\n",
    "        ]\n",
    "\n",
    "    def next_tuple(self):\n",
    "        sentence = random.choice(self.sentences)\n",
    "        self.emit([sentence])\n",
    "\n",
    "class SplitSentenceBolt(Bolt):\n",
    "    outputs = ['word']\n",
    "\n",
    "    def process(self, tup):\n",
    "        sentence = tup.values[0]\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            self.emit([word])\n",
    "\n",
    "class WordCountBolt(Bolt):\n",
    "    outputs = ['word', 'count']\n",
    "\n",
    "    def initialize(self, stormconf, context):\n",
    "        self.counts = {}\n",
    "\n",
    "    def process(self, tup):\n",
    "        word = tup.values[0]\n",
    "        if word not in self.counts:\n",
    "            self.counts[word] = 0\n",
    "        self.counts[word] += 1\n",
    "        self.emit([word, self.counts[word]])\n",
    "        self.log(f'{word}: {self.counts[word]}')\n",
    "\n",
    "class WordCountTopology(Topology):\n",
    "    random_sentence_spout = RandomSentenceSpout.spec()\n",
    "    split_sentence_bolt = SplitSentenceBolt.spec(inputs=[random_sentence_spout])\n",
    "    word_count_bolt = WordCountBolt.spec(inputs=[split_sentence_bolt])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(WordCountTopology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "import happybase\n",
    "\n",
    "# 连接到Hive\n",
    "hive_conn = hive.Connection(host='localhost', port=10000, username='your_username')\n",
    "hive_cursor = hive_conn.cursor()\n",
    "\n",
    "# 创建Hive表\n",
    "hive_cursor.execute(\"CREATE TABLE IF NOT EXISTS users (id INT, name STRING, age INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n",
    "hive_cursor.execute(\"LOAD DATA LOCAL INPATH '/path/to/your/data.csv' INTO TABLE users\")\n",
    "\n",
    "# 查询Hive表\n",
    "hive_cursor.execute(\"SELECT * FROM users\")\n",
    "for result in hive_cursor.fetchall():\n",
    "    print(result)\n",
    "\n",
    "# 关闭Hive连接\n",
    "hive_conn.close()\n",
    "\n",
    "# 连接到HBase\n",
    "hbase_conn = happybase.Connection('localhost')\n",
    "table = hbase_conn.table('users')\n",
    "\n",
    "# 插入数据到HBase\n",
    "table.put(b'row1', {b'cf:name': b'Alice', b'cf:age': b'30'})\n",
    "table.put(b'row2', {b'cf:name': b'Bob', b'cf:age': b'25'})\n",
    "\n",
    "# 查询HBase表\n",
    "for key, data in table.scan():\n",
    "    print(key, data)\n",
    "\n",
    "# 关闭HBase连接\n",
    "hbase_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "import happybase\n",
    "\n",
    "# 连接到Hive\n",
    "hive_conn = hive.Connection(host='localhost', port=10000, username='your_username')\n",
    "hive_cursor = hive_conn.cursor()\n",
    "\n",
    "# 创建Hive表\n",
    "hive_cursor.execute(\"CREATE TABLE IF NOT EXISTS hdfs_data (id INT, name STRING, age INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n",
    "\n",
    "# 将HDFS数据加载到Hive表中\n",
    "hive_cursor.execute(\"LOAD DATA INPATH '/path/to/hdfs/data.csv' INTO TABLE hdfs_data\")\n",
    "\n",
    "# 查询Hive表\n",
    "hive_cursor.execute(\"SELECT * FROM hdfs_data\")\n",
    "for result in hive_cursor.fetchall():\n",
    "    print(result)\n",
    "\n",
    "# 创建HBase连接\n",
    "hbase_conn = happybase.Connection('localhost')\n",
    "table = hbase_conn.table('hbase_table')\n",
    "\n",
    "# 将Hive表的数据加载到HBase表中\n",
    "hive_cursor.execute(\"SELECT * FROM hdfs_data\")\n",
    "for row in hive_cursor.fetchall():\n",
    "    row_key = f'row{row[0]}'.encode('utf-8')\n",
    "    table.put(row_key, {b'cf:name': row[1].encode('utf-8'), b'cf:age': str(row[2]).encode('utf-8')})\n",
    "\n",
    "# 查询HBase表\n",
    "for key, data in table.scan():\n",
    "    print(key, data)\n",
    "\n",
    "# 关闭连接\n",
    "hive_conn.close()\n",
    "hbase_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "# 创建HDFS连接\n",
    "hdfs_client = hdfs.connect(host='localhost', port=9000, user='your_username')\n",
    "\n",
    "# 创建一个示例数据\n",
    "data = b\"Hello, HDFS!\"\n",
    "\n",
    "# 将数据写入HDFS文件\n",
    "with hdfs_client.open('/path/to/hdfs/file.txt', 'wb') as f:\n",
    "    f.write(data)\n",
    "\n",
    "# 读取HDFS文件内容\n",
    "with hdfs_client.open('/path/to/hdfs/file.txt', 'rb') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# 关闭HDFS连接\n",
    "hdfs_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "import pyarrow as pa\n",
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "# 示例数据\n",
    "data = [\n",
    "    {'id': 1, 'name': 'Alice', 'age': 30},\n",
    "    {'id': 2, 'name': 'Bob', 'age': 25},\n",
    "    {'id': 3, 'name': 'Charlie', 'age': 35}\n",
    "]\n",
    "\n",
    "# 将数据存入HBase\n",
    "hbase_conn = happybase.Connection('localhost')\n",
    "table = hbase_conn.table('users')\n",
    "\n",
    "for row in data:\n",
    "    row_key = f'row{row[\"id\"]}'.encode('utf-8')\n",
    "    table.put(row_key, {\n",
    "        b'cf:name': row['name'].encode('utf-8'),\n",
    "        b'cf:age': str(row['age']).encode('utf-8')\n",
    "    })\n",
    "\n",
    "# 关闭HBase连接\n",
    "hbase_conn.close()\n",
    "\n",
    "# 将数据存入HDFS\n",
    "hdfs_client = hdfs.connect(host='localhost', port=9000, user='your_username')\n",
    "\n",
    "# 创建一个示例数据文件内容\n",
    "file_content = \"id,name,age\\n\" + \"\\n\".join([f'{row[\"id\"]},{row[\"name\"]},{row[\"age\"]}' for row in data])\n",
    "\n",
    "# 将数据写入HDFS文件\n",
    "with hdfs_client.open('/path/to/hdfs/users.csv', 'wb') as f:\n",
    "    f.write(file_content.encode('utf-8'))\n",
    "\n",
    "# 关闭HDFS连接\n",
    "hdfs_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "### 如何本地安装HBase环境\n",
    "\n",
    "1. **下载HBase**:\n",
    "    - 访问 [HBase官网](https://hbase.apache.org/downloads.html) 下载最新版本的HBase。\n",
    "    - 选择一个镜像站点并下载HBase的二进制文件（例如：`hbase-2.4.8-bin.tar.gz`）。\n",
    "\n",
    "2. **解压文件**:\n",
    "    ```bash\n",
    "    tar -xzf hbase-2.4.8-bin.tar.gz\n",
    "    cd hbase-2.4.8\n",
    "    ```\n",
    "\n",
    "3. **配置HBase**:\n",
    "    编辑`conf/hbase-site.xml`文件，添加以下配置：\n",
    "    ```xml\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>hbase.rootdir</name>\n",
    "            <value>file:///path/to/hbase</value>\n",
    "        </property>\n",
    "        <property>\n",
    "            <name>hbase.zookeeper.property.dataDir</name>\n",
    "            <value>/path/to/zookeeper</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "    ```\n",
    "\n",
    "4. **启动HBase**:\n",
    "    ```bash\n",
    "    ./bin/start-hbase.sh\n",
    "    ```\n",
    "\n",
    "5. **验证安装**:\n",
    "    通过HBase shell验证安装是否成功：\n",
    "    ```bash\n",
    "    ./bin/hbase shell\n",
    "    ```\n",
    "\n",
    "通过以上步骤，你已经在本地成功安装并运行了HBase环境。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
